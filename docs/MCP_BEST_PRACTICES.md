# MCP Server Best Practices Guide

**Learn from FastIntercomMCP: How to Build High-Performance MCP Servers**

Based on our experience building FastIntercomMCP and analyzing 50+ existing MCP implementations, this guide provides battle-tested best practices for creating production-ready MCP servers.

## üéØ Core Design Principles

### 1. **Performance-First Architecture**
Most MCP servers are API wrappers that inherit the performance limitations of external services. High-performance MCP servers require local optimization.

#### ‚ùå **API Wrapper Pattern (Slow)**
```python
# What most MCP servers do (DON'T do this for performance)
class SlowMCPServer:
    async def search_data(self, query):
        # Every query hits external API
        response = await external_api.search(query)
        return response.json()
        
# Result: Slow queries, rate limiting, high costs
```

#### ‚úÖ **Local-First Pattern (Fast)**
```python
# What FastIntercomMCP does (DO this for performance)
class FastMCPServer:
    async def search_data(self, query):
        # Query local optimized database
        result = await self.database.search(query)
        return result
        
    async def sync_data(self):
        # Background sync from external API
        data = await external_api.get_all_data()
        await self.database.store(data)
        
# Result: Sub-50ms queries, unlimited scale, low costs
```

### 2. **Sync-First Design Pattern**
**Key insight**: Separate data ingestion from data querying for maximum performance.

```python
class HighPerformanceMCPArchitecture:
    """
    Two-layer architecture:
    1. Sync Layer: API ‚Üí Database (background, scheduled)  
    2. Query Layer: Database ‚Üí MCP Client (real-time, fast)
    """
    
    def __init__(self):
        self.sync_engine = DataSyncEngine()    # Handles external API
        self.query_engine = QueryEngine()      # Handles MCP requests
        self.database = OptimizedDatabase()    # Local storage
```

### 3. **Domain-Specific Optimization**
Generic database MCP servers miss domain-specific optimization opportunities.

#### Generic vs. Optimized Schema Design
```sql
-- ‚ùå Generic approach (MCP-Alchemy style)
CREATE TABLE data (
    id TEXT,
    content JSONB,
    created_at TIMESTAMP
);

-- ‚úÖ Domain-optimized approach (FastIntercomMCP style)
CREATE TABLE conversations (
    id TEXT PRIMARY KEY,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL,
    state VARCHAR(20) NOT NULL,
    customer_id TEXT,
    tags TEXT[],
    -- Domain-specific indexes for conversation analytics
    CONSTRAINT conversations_state_check CHECK (state IN ('open', 'closed', 'snoozed'))
);

-- Optimized indexes for common conversation queries
CREATE INDEX idx_conversations_time_state ON conversations (created_at DESC, state);
CREATE INDEX idx_conversations_customer ON conversations (customer_id);
CREATE INDEX idx_conversations_tags ON conversations USING GIN (tags);
```

## üèóÔ∏è Architecture Best Practices

### 1. **Layered Architecture Pattern**
```python
"""
Recommended 4-layer architecture for high-performance MCP servers:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           MCP Protocol Layer            ‚îÇ  ‚Üê JSON-RPC, tool registration
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ           Business Logic Layer          ‚îÇ  ‚Üê Domain-specific operations  
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ           Data Access Layer             ‚îÇ  ‚Üê Database queries, caching
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ           Storage Layer                 ‚îÇ  ‚Üê PostgreSQL, Redis, etc.
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""

# Implementation example:
class MCPServer:
    def __init__(self):
        self.protocol = MCPProtocolHandler()     # Layer 1
        self.business = BusinessLogicEngine()    # Layer 2  
        self.data = DataAccessLayer()           # Layer 3
        self.storage = DatabaseClient()         # Layer 4
```

### 2. **Async-First Implementation**
All I/O operations should be async for maximum concurrency.

```python
# ‚úÖ Proper async implementation
class AsyncMCPServer:
    async def handle_tool_call(self, tool_name: str, params: dict):
        # All database operations are async
        async with self.db_pool.acquire() as conn:
            results = await conn.fetch(query, *params)
            
        # All external API calls are async  
        async with httpx.AsyncClient() as client:
            response = await client.get(url)
            
        return self.format_response(results)

# ‚ùå Blocking implementation (don't do this)
class BlockingMCPServer:
    def handle_tool_call(self, tool_name: str, params: dict):
        # Blocks entire server for each request
        results = blocking_db_query(query)
        response = blocking_api_call(url)
        return results
```

### 3. **Connection Pooling & Resource Management**
```python
# ‚úÖ Proper resource management
class ResourceOptimizedMCP:
    def __init__(self):
        # Database connection pool
        self.db_pool = asyncpg.create_pool(
            database_url,
            min_size=5,
            max_size=20,
            command_timeout=60
        )
        
        # HTTP client with connection reuse
        self.http_client = httpx.AsyncClient(
            limits=httpx.Limits(max_connections=100),
            timeout=httpx.Timeout(30.0)
        )
        
    async def cleanup(self):
        await self.db_pool.close()
        await self.http_client.aclose()
```

## üìä Performance Optimization Patterns

### 1. **Query Optimization Strategy**
```python
class QueryOptimizer:
    """
    3-tier query optimization:
    1. Query result caching (Redis/in-memory)
    2. Database query optimization (indexes, views)
    3. Response pagination and streaming
    """
    
    async def search_with_cache(self, query: str, filters: dict):
        # 1. Check cache first
        cache_key = self.generate_cache_key(query, filters)
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
            
        # 2. Execute optimized database query
        result = await self.execute_optimized_query(query, filters)
        
        # 3. Cache for future requests
        await self.cache.set(cache_key, result, ttl=300)
        return result
        
    async def execute_optimized_query(self, query: str, filters: dict):
        # Use prepared statements for common patterns
        if query in self.prepared_statements:
            return await self.db.fetch(self.prepared_statements[query], *filters)
            
        # Dynamic query with proper parameterization
        sql, params = self.build_query(query, filters)
        return await self.db.fetch(sql, *params)
```

### 2. **Caching Strategy**
```python
class MultiLevelCaching:
    """
    Implement multiple cache levels for different data types:
    - L1: In-memory (fastest, limited size)
    - L2: Redis (fast, larger size) 
    - L3: Materialized views (persistent, computed)
    """
    
    def __init__(self):
        self.l1_cache = {}  # In-memory LRU cache
        self.l2_cache = redis.Redis()  # Redis cache
        # L3: Materialized views in database
        
    async def get_with_multilevel_cache(self, key: str):
        # L1: Check in-memory cache
        if key in self.l1_cache:
            return self.l1_cache[key]
            
        # L2: Check Redis cache
        l2_result = await self.l2_cache.get(key)
        if l2_result:
            self.l1_cache[key] = l2_result  # Promote to L1
            return l2_result
            
        # L3: Check materialized views / compute
        result = await self.compute_result(key)
        
        # Store in both cache levels
        self.l1_cache[key] = result
        await self.l2_cache.set(key, result, ex=3600)
        
        return result
```

### 3. **Batching and Streaming**
```python
class BatchingMCPServer:
    """
    Implement batching for:
    1. Database operations (bulk inserts/updates)
    2. External API calls (batch requests when supported)
    3. MCP responses (streaming for large results)
    """
    
    async def bulk_sync_data(self, data_items: List[dict]):
        # Batch database operations for efficiency
        async with self.db_pool.acquire() as conn:
            async with conn.transaction():
                await conn.executemany(
                    "INSERT INTO table VALUES ($1, $2, $3) ON CONFLICT (id) DO UPDATE...",
                    [(item['id'], item['data'], item['timestamp']) for item in data_items]
                )
                
    async def stream_large_results(self, query: str):
        # Stream results for large datasets
        async with self.db_pool.acquire() as conn:
            async for record in conn.cursor(query):
                yield self.format_record(record)
```

## üîå MCP Protocol Best Practices

### 1. **Tool Definition Best Practices**
```python
# ‚úÖ Well-designed MCP tool
{
    "name": "search_conversations",
    "description": "Search conversations with advanced filtering and analytics",
    "inputSchema": {
        "type": "object",
        "properties": {
            # Clear, specific parameters
            "query": {
                "type": "string",
                "description": "Natural language search query or keywords",
                "examples": ["authentication issues", "billing problems"]
            },
            "timeframe": {
                "type": "object", 
                "description": "Time range for search",
                "properties": {
                    "start_date": {"type": "string", "format": "date-time"},
                    "end_date": {"type": "string", "format": "date-time"},
                    "relative": {
                        "type": "string", 
                        "enum": ["today", "yesterday", "last_7_days", "last_30_days"],
                        "description": "Relative time period"
                    }
                }
            },
            # Sensible defaults and limits
            "limit": {
                "type": "integer", 
                "default": 50, 
                "minimum": 1, 
                "maximum": 1000,
                "description": "Maximum number of results to return"
            }
        },
        "required": []  # Make most parameters optional with defaults
    }
}
```

### 2. **Error Handling Standards**
```python
class MCPErrorHandler:
    """
    Implement consistent error handling across all tools:
    1. Proper JSON-RPC error codes
    2. Detailed error messages
    3. Graceful degradation
    4. Comprehensive logging
    """
    
    async def handle_tool_call(self, tool_name: str, params: dict):
        try:
            # Validate parameters
            validated_params = self.validate_params(tool_name, params)
            
            # Execute tool
            result = await self.execute_tool(tool_name, validated_params)
            
            return {
                "jsonrpc": "2.0",
                "id": self.request_id,
                "result": result
            }
            
        except ValidationError as e:
            # Parameter validation errors
            return self.error_response(
                code=-32602,  # Invalid params
                message=f"Invalid parameters: {e}",
                data={"tool": tool_name, "params": params}
            )
            
        except DatabaseError as e:
            # Database errors
            logger.error(f"Database error in {tool_name}: {e}")
            return self.error_response(
                code=-32603,  # Internal error
                message="Database operation failed",
                data={"tool": tool_name, "retry": True}
            )
            
        except Exception as e:
            # Unexpected errors
            logger.exception(f"Unexpected error in {tool_name}: {e}")
            return self.error_response(
                code=-32603,
                message="Internal server error",
                data={"tool": tool_name}
            )
```

### 3. **Response Formatting Standards**
```python
class ResponseFormatter:
    """
    Consistent response formatting across all tools:
    1. Structured data with metadata
    2. Performance metrics included
    3. Pagination information
    4. Human-readable summaries
    """
    
    def format_search_response(self, results: List[dict], query_info: dict):
        return {
            "summary": {
                "total_results": len(results),
                "query_time_ms": query_info.get("duration_ms"),
                "data_source": "local_database",
                "cache_hit": query_info.get("cache_hit", False)
            },
            "results": results,
            "pagination": {
                "page": query_info.get("page", 1),
                "per_page": query_info.get("limit", 50),
                "has_more": len(results) == query_info.get("limit", 50)
            },
            "metadata": {
                "query": query_info.get("query"),
                "filters_applied": query_info.get("filters"),
                "last_updated": self.get_last_sync_time()
            }
        }
```

## üîê Security Best Practices

### 1. **Input Validation & SQL Injection Prevention**
```python
class SecureQueryBuilder:
    """
    Always use parameterized queries and input validation:
    """
    
    def build_safe_query(self, filters: dict) -> Tuple[str, List]:
        # ‚úÖ Parameterized query building
        query_parts = ["SELECT * FROM conversations WHERE 1=1"]
        params = []
        
        if filters.get("customer_id"):
            query_parts.append("AND customer_id = $%d" % (len(params) + 1))
            params.append(filters["customer_id"])
            
        if filters.get("date_range"):
            query_parts.append("AND created_at BETWEEN $%d AND $%d" % (len(params) + 1, len(params) + 2))
            params.extend([filters["date_range"]["start"], filters["date_range"]["end"]])
            
        # ‚ùå NEVER do string formatting (SQL injection risk)
        # query = f"SELECT * FROM conversations WHERE customer_id = '{customer_id}'"
        
        return " ".join(query_parts), params
        
    async def validate_input(self, tool_name: str, params: dict):
        # Comprehensive input validation
        schema = self.get_tool_schema(tool_name)
        try:
            validated = jsonschema.validate(params, schema)
            return validated
        except jsonschema.ValidationError as e:
            raise ValidationError(f"Invalid input: {e.message}")
```

### 2. **Authentication & Authorization**
```python
class MCPSecurityHandler:
    """
    Implement proper authentication and authorization:
    """
    
    def __init__(self):
        self.api_keys = self.load_api_keys()
        self.rate_limiters = {}
        
    async def authenticate_request(self, headers: dict):
        # API key authentication
        api_key = headers.get("Authorization", "").replace("Bearer ", "")
        if not api_key or api_key not in self.api_keys:
            raise AuthenticationError("Invalid API key")
            
        return self.api_keys[api_key]  # Returns user info
        
    async def check_rate_limit(self, client_id: str):
        # Per-client rate limiting
        if client_id not in self.rate_limiters:
            self.rate_limiters[client_id] = TokenBucket(
                capacity=100,    # 100 requests
                refill_rate=10   # per 10 seconds
            )
            
        if not self.rate_limiters[client_id].consume():
            raise RateLimitError("Rate limit exceeded")
```

## üìà Monitoring & Observability

### 1. **Comprehensive Logging**
```python
import structlog

class MCPLogger:
    """
    Structured logging for all MCP operations:
    """
    
    def __init__(self):
        self.logger = structlog.get_logger()
        
    async def log_tool_call(self, tool_name: str, params: dict, result: dict, duration_ms: float):
        self.logger.info(
            "mcp_tool_called",
            tool=tool_name,
            params_hash=hashlib.md5(str(params).encode()).hexdigest()[:8],
            result_count=len(result.get("results", [])),
            duration_ms=duration_ms,
            cache_hit=result.get("cache_hit", False),
            database_query_count=result.get("db_queries", 0)
        )
        
    async def log_sync_operation(self, operation: str, records_processed: int, duration_ms: float):
        self.logger.info(
            "sync_operation",
            operation=operation,
            records_processed=records_processed,
            duration_ms=duration_ms,
            records_per_second=records_processed / (duration_ms / 1000)
        )
```

### 2. **Performance Metrics**
```python
class PerformanceMonitor:
    """
    Track and expose performance metrics:
    """
    
    def __init__(self):
        self.metrics = {
            "total_requests": 0,
            "avg_response_time": 0,
            "cache_hit_rate": 0,
            "active_connections": 0
        }
        
    async def track_request(self, duration_ms: float, cache_hit: bool):
        self.metrics["total_requests"] += 1
        
        # Rolling average response time
        current_avg = self.metrics["avg_response_time"]
        total_requests = self.metrics["total_requests"]
        self.metrics["avg_response_time"] = (
            (current_avg * (total_requests - 1) + duration_ms) / total_requests
        )
        
        # Cache hit rate
        if cache_hit:
            self.cache_hits += 1
        self.metrics["cache_hit_rate"] = self.cache_hits / total_requests
        
    def get_health_status(self):
        return {
            "status": "healthy" if self.metrics["avg_response_time"] < 100 else "degraded",
            "metrics": self.metrics,
            "last_sync": self.get_last_sync_time(),
            "database_status": self.check_database_health()
        }
```

## üöÄ Deployment Best Practices

### 1. **Docker & Container Optimization**
```dockerfile
# Multi-stage Dockerfile for production optimization
FROM python:3.11-slim as builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Production stage
FROM python:3.11-slim as production

# Copy only necessary files
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# Create non-root user
RUN groupadd -r mcp && useradd -r -g mcp mcp
USER mcp

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

EXPOSE 8000
CMD ["python", "-m", "src.mcp.server"]
```

### 2. **Production Configuration**
```yaml
# docker-compose.production.yml
version: '3.8'
services:
  mcp-server:
    image: your-mcp-server:latest
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/mcp
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=INFO
      - WORKERS=4
    depends_on:
      - db
      - redis
      
  db:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=mcp
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    deploy:
      resources:
        limits:
          memory: 1G
          
  redis:
    image: redis:7-alpine
    deploy:
      resources:
        limits:
          memory: 256M
```

## üéØ Common Pitfalls to Avoid

### 1. **Performance Anti-Patterns**
```python
# ‚ùå DON'T: API wrapper without optimization
class SlowMCPServer:
    async def search(self, query):
        # Every request hits external API - slow!
        return await external_api.search(query)

# ‚ùå DON'T: Blocking operations  
def handle_request(self, params):
    # Blocks entire server
    result = blocking_database_call(params)
    return result

# ‚ùå DON'T: No connection pooling
async def query_database(self, sql):
    # Creates new connection for every query
    conn = await asyncpg.connect(database_url)
    result = await conn.fetch(sql)
    await conn.close()
    return result
```

### 2. **Security Anti-Patterns**
```python
# ‚ùå DON'T: String formatting in SQL
def build_query(self, user_input):
    # SQL injection vulnerability!
    return f"SELECT * FROM users WHERE name = '{user_input}'"

# ‚ùå DON'T: No input validation
async def handle_tool_call(self, tool_name, params):
    # Directly use user input without validation
    return await self.database.query(params["sql"])

# ‚ùå DON'T: Expose internal errors
except Exception as e:
    # Don't expose internal implementation details
    return {"error": str(e)}  # Could leak sensitive info
```

### 3. **Architecture Anti-Patterns**
```python
# ‚ùå DON'T: Monolithic tool handlers
async def giant_search_tool(self, params):
    # 500+ lines of mixed concerns
    # Database logic + business logic + formatting all in one function
    
# ‚ùå DON'T: No error boundaries
async def handle_request(self, request):
    # One error breaks entire server
    result = await self.process_request(request)
    return result  # No error handling

# ‚ùå DON'T: Tight coupling
class MCPServer:
    def __init__(self):
        # Directly instantiate dependencies
        self.database = PostgreSQLClient()  # Can't test or swap
        self.cache = RedisClient()          # Hard to mock
```

## üìö Additional Resources

### Recommended Libraries & Frameworks
- **FastMCP** (`pip install fastmcp`) - Python MCP framework
- **AsyncPG** (`pip install asyncpg`) - High-performance PostgreSQL client
- **Structlog** (`pip install structlog`) - Structured logging
- **Pydantic** (`pip install pydantic`) - Data validation and settings
- **HTTPx** (`pip install httpx`) - Async HTTP client

### MCP Ecosystem Resources
- **Official MCP Specification**: https://spec.modelcontextprotocol.io/
- **MCP Python SDK**: https://github.com/modelcontextprotocol/python-sdk
- **Awesome MCP Servers**: https://github.com/punkpeye/awesome-mcp-servers
- **FastIntercomMCP** (our implementation): [GitHub link when published]

### Database Optimization Resources  
- **PostgreSQL Performance**: https://wiki.postgresql.org/wiki/Performance_Optimization
- **Database Indexing**: https://use-the-index-luke.com/
- **Query Optimization**: https://explain.depesz.com/

## üéØ Conclusion

Building high-performance MCP servers requires:

1. **Architecture**: Local-first with background sync
2. **Performance**: Async operations, connection pooling, caching
3. **Security**: Input validation, parameterized queries, rate limiting  
4. **Monitoring**: Structured logging, health checks, metrics
5. **Deployment**: Containerization, resource limits, scaling

**Key insight**: Most MCP servers are API wrappers. The real value comes from local optimization and domain-specific intelligence.

FastIntercomMCP demonstrates these principles in practice - use it as a reference implementation for your own high-performance MCP servers.

---

**By following these best practices, you can build MCP servers that deliver 100x performance improvements over simple API wrappers while maintaining production-grade reliability and security.**